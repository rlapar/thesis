\chapter{Implementation}

As a part of this thesis, we designed a simple tool able to work with our dataset and execute various tasks over it. The main idea was to unify the interface for models, so many different models can be run just via command line. This proved to be essential when running many models concurrently in Metacentrum or in the local machine. 

The whole tool is written in Python 3.6 and it is accessible via one command-line interface.

\section{Used technologies}

\subsection{Pandas}

To work with CSV files of a length of more than 30 million lines, one does need to use the appropriate framework for it. We chose the pandas\cite{pandas} library. Actively supported today by a community, it is a BSD-licensed library providing high-performance, low-level optimized and easy-to-use data structures for data analysis for \textit{Python} programming language. Pandas stores its data in so-called dataframes\cite{pandas-df}, which are working on the same principle as in the \textit{R} language. They can store two-dimensional size-mutable heterogeneous tabular data and allow arithmetic operations align on both row and column. Furthermore, pandas can work with with huge CSV files using python generators.

\subsection{TensorFlow}

TensorFlow\cite{tensorflow} is an open source software library for high performance numerical computation. Tensorflow was developed by the Google Brain team for internal Google use. It was released under the Apache 2.0 open source license on November 9, 2015 Its flexible architecture allows easy deployment of computation across a variety of platforms (CPUs, GPUs, TPUs), and from desktops to clusters of servers to mobile and edge devices. Originally developed by researchers and engineers from the Google Brain team within Googleâ€™s AI organization, it comes with strong support for machine learning and deep learning and the flexible numerical computation core is used across many other scientific domains. Its core is written in C++ and CUDA, thus it is highly optimized for parallel computations on GPU.

\subsection{Keras}

Keras\cite{keras} is arguably one of the most popular python tensorflow frontends for a quick and simple definition of networks. In comparison to tensorflow, Keras is very layer-oriented and has an intuitive user friendly API. All models, that we trained were defined over Keras framework. The simplicity of Keras framework can be shown in an example of a concrete model, that we used. The whole model is defined in just 5 lines of code:

\begin{minted}[ framesep=2mm,
                autogobble,
                frame=lines]{python}
import keras
inputs = keras.layers.Input(shape=(input_dimension,))
x = keras.layers.Dense(1024, activation='relu')(inputs)
outputs = keras.layers.Dense(output_dimension, activation='softmax')(x)
model = keras.models.Model(inputs=inputs, outputs=outputs)
model.compile(loss=keras.losses.categorical_crossentropy,
              optimizer=keras.optimizers.Adadelta(),
              metrics=['acc'])

\end{minted}

\subsection{Scikit-learn}
 
Before trying any neural network models, we tried to use simple models first. We chose the sklearn\cite{scikit} package, the native package for basic machine learning and evaluation. We used classifiers such as KNN, Naive Bayes, decision trees or support vector classifiers\footnote{List of all possible classifiers can be found in \url{http://scikit-learn.org/stable/supervised\_learning.html\#supervised-learning}}. 

\subsection{Docker}

Initially, the tool was developed on the local Linux machine, because it was dependent on third-party Unix binaries, as well as it was later deployed to Metacentrum cloud, which is Unix native. However, we needed to develop and run it also on other architectures. One approach is to install virtual machines on the desired host, setup Unix environment and then run the tool. The better, more modern approach would be to use containers, namely Docker\cite{docker}.

Docker encapsulates the application, running environment, dependencies and architecture into a self-contained unit that can run anywhere. It is a more lightweight virtual machine because containers in comparison to virtual machines share the host system's kernel with other containers\cite{docker-blog}. By defining dockerfile, we provided any user the option to run this tool on its own local host.

\subsection{Click}

As our tool provides only command line interface, we chose the Click package\cite{click}, a command line interface creation kit. It provides decorator options for defining arguments and options with validation and automatic help generation. As a framework, it suited the necessary needs for the application and sped up the development process.

\section{Tasks}

During the implementation process, several distinct tasks were implemented in order to work with the original dataset, prepare the data for models to train and then classify. Figure \ref{figure-model} shows the flow of the application and its data. We obtained a dataset in the form of around 2000 CSV files. First of all, we wanted to automate extracting the keys from distinct files and set up class labels to it. 

Therefore, \textbf{SCAN} task was implemented. We used it mainly to transform given dataset into more compact one, which could be used more efficiently for analysis and generating new ones.

The newly generated compact dataset was then subjected to full analysis of features. For this purpose \textbf{ANALYZE} task was implemented. We used it to generate the relative frequency of features, that we presented in the chapter \label{chapter-analysis}.

Another task \textbf{GENERATE} was used to prepare custom generated datasets for machine learning models. Its main purpose was to be able to generate datasets with different class labels, replicate keys in training dataset, skip group, skip files, merge several groups into one and others. We used this task throughout the whole training process to create datasets for models and tune them if the model was struggling in training.

The main task \textbf{TRAIN} was applied to the custom generated datasets. Its main purpose was to be able to feed data into the model, monitor performance and report the results of the training. Models are sharing the same interface. This allows to implement new model relatively simply and use the whole task for any model.

Models that are performing well can be used, evaluated and tested via \textbf{CLASSIFY} task using the same shared core with \textbf{TRAIN}.

\begin{figure}[H]

\centering
\label{figure-model}
\includegraphics[width=0.65\textwidth]{tex/images/thesis_model}
\caption{Diagram of the application flow shows 5 distinct tasks (SCAN, ANALYZE, GENERATE, TRAIN, CLASSIFY) that we used to work with our dataset and perform machine learning on them.}

\end{figure}

\section{SCAN task}

The original dataset was in the form of a directory hierarchy, where there was no specific namespace for different key length and sources. We needed a task to be able to create groups of sources, that would be corresponding to the same label. The using this grouping generate a new dataset, which is better structured and easier to analyze.

\textit{SCAN} task is using simple string matching with required (positive) keywords and forbidden (negative) keywords. Then it iteratively loops through all CSV files in given root directory and looks for CSV files that contain any positive keyword and not containing any negative keyword. Keywords are passed to the task via JSON structure. In Here is an example of a JSON filter we defined over the original dataset, which scan all sources from group 13 with bitlength of 1024: 

\begin{minted}[ framesep=2mm,
                autogobble,
                frame=lines]{json}
{
    ...
    "13": {
        "required": [
            "Botan_1-5-6",
            "Botan_1-11-29",
            ...
            "WolfSSL_3-9-0",
            "WolfSSL_3-10-2"
        ],
        "forbidden": [
            "PGP_SDK_4_FIPS",
            "Libgcrypt_1-7-6_FIPS",
            "512",
            "2048"
        ]
    }
}

\end{minted}

\noindent
As a result, we obtain a JSON file, which contains groups of sources with given label and a list of individual sources with the number of keys in them. This JSON file is used further by \textit{GENERATE} task. Below we show the example of a generated JSON file from the previous filter:

\begin{minted}[ framesep=2mm,
                autogobble,
                frame=lines]{json}
{
    ...
     "13": {
            "group_name": "13",
            "keys_total": 31911043,
            "sources": [
                {
                    "length": 50001,
                    "name": "Feitian JavaCOS A22",
                    "path": "/Card/Feitian JavaCOS A22 1024b/UnknownICSN 1.csv",
                    "read_lines": 0
                },
                ...
            ]
     }
}

\end{minted}

\section{ANALYZE task}

The problem with working with huge CSV files is memory consumption, as we usually are not able to load all keys into RAM at once. Fortunately, pandas\cite{pandas} library offers reading and processing such files in chunks of lines. 

When analyzing a huge file, we iterate it in chunks, extracting values for features from every key and then just incrementing the corresponding counter. In the end, we print the analysis to the user.

\begin{figure}[h]

\centering
\includegraphics[width=0.5\textwidth]{tex/images/analyze_task}
\caption{\textit{ANALYZE} task flow.}

\end{figure}

Using \textit{SCAN} and \textit{GENERATE} we transformed the dataset to fit our purposes. We extracted all keys from each of available 64 sources of different key lengths and merged them all into single CSV files\footnote{i.e. for source 01 with we obtained 3 files, each with 512/1024/2048 keys respectively}. 

Having the dataset decoupled into several smaller CSV files, we could actually perform an analysis on each of these files distributively in the CESNET cloud.

\subsection{Feature Maker}

When analyzing the dataset, we obtained features to use in machine learning. Having them hardcoded would not be the best approach, as new features may occur or some features may not be used for some specific model. 

That is why we designed the Feature Maker. It is essentially a class, that is dedicated to the transformation of a raw key into the specific feature vector. Features may be added during runtime dynamically, usually via a configuration file. It gives us freedom of choosing a different set of features with every model (or during the analysis as well). Also, adding new feature does not change any other code, but the Feature Maker class itself. The features implemented so far:

\begin{itemize}

\item \textit{modulo} - can return either binary vector class (useful for machine learning) or remainder as a decimal number (useful for analysis)
\item \textit{bit} - returns specific bit (useful for mask in machine learning and analysis)
\item \textit{line} - returns whole line (shortcut of taking every bit)
\item \textit{xor} - return xor of two specified bits

\end{itemize}

\noindent
Using this class is a question of couple lines of code. In the following simplified example, we define the feature maker, assigned three features to it (modulo 3, modulo 7 and 38-th LSB), then call the \texttt{get\_features\_from\_keys} which extract features for us. Adding or removing features can be performed between 2nd and 4th line of code:

\begin{minted}[ framesep=2mm,
                autogobble,
                frame=lines]{python}

fm = FeatureMaker()
fm.add('mod3')
fm.add('mod', {'n': 7})
fm.add('bit', {'i': 37})
features = fm.get_features_from_keys(keys)

\end{minted}

\section{GENERATE task}
\section{TRAIN task}
\section{CLASSIFY task}

\section{Docker}
\section{Metacentrum}