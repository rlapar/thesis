\chapter{Results}

Various models were tried using our application. The results were then compared to the existing ones from the CROCS research. We started with simple models from \texttt{scikit} library and smaller datasets (of around 65000 samples). We were able to reach accuracy ranging from 7.5 - 33.5 \% on average (comparable to \cite{thesis_sekan}):

\begin{figure}[H]

\centering

\begin{tabular}{|l|l|}
\hline 
Classifier & $\sim$ accuracy \\
\hline 
RadiusNeighborsClassifier & 7.50 \% \\
QuadraticDiscriminantAnalysis & 7.68 \% \\
ExtraTreeClassifier & 12.62 \% \\
MLPClassifier & 12.65 \% \\
DecisionTreeClassifier & 12.71 \% \\
KNeighborsClassifier & 16.92 \% \\
SGDClassifier & 20.70 \% \\
PassiveAggressiveClassifier & 21.72 \% \\
AdaBoostClassifier & 22.34 \% \\
GaussianNB & 27.51 \% \\
MultinomialNB & 28.22 \% \\
LinearSVC & 29.93 \% \\
LinearDiscriminantAnalysis & 30.81 \% \\
NuSVC & 30.93 \% \\
RidgeClassifier & 31.14 \% \\
BernoulliNB & 31.21 \% \\
RidgeClassifierCV & 31.22 \% \\
SVC & 32.57 \% \\
BaggingClassifier & 32.74 \% \\
RandomForestClassifier & 33.04 \% \\
ExtraTreesClassifier & 33.26 \% \\
GradientBoostingClassifier & 33.57 \% \\ 
\hline
\end{tabular}

\end{figure}

\section{Multi layer perceptrons}

Before any models were trained, we performed an analysis of the dataset, applied feature engineering and extracted the features described in the subsection \ref{feature-engineering}. We took the whole key (as a binary vector of respective key length 512 / 1024 / 2048) and all its moduli up to 30. Modulus was also represented as a binary vector. For example, the feature modulo 7 could result in 6 different vectors, each having a different index set to 1 and representing one of 6 possible remainders. In the end, we obtained one large binary vector which we fed to the models.

\subsection*{Grid search}

Apart from the features extracted from the analysis, all the other tried features seem to have a uniform distribution within its domain. On the first look, the model to be used is not apparent, so we decided to use the \textbf{grid search}\footnote{described in section \ref{section-grid-search}} for finding the most suitable topology and configuration. 

We used this approach mainly to find the optimal choice of the topology (meaning the number of hidden layers and the number of neurons within each layer), the selected activations in each layer and optimizers.

We were gradually testing topologies from zero up to two hidden layers, with the number of neurons being the powers of 2 starting from 8 up to 4096. The activation functions used were sigmoid, hyperbolic tangent, ReLU and leaky ReLU. This gives us around 160 differenandt configurations to run, which is an ideal use case for cloud computing. Running for a couple of days, we were able to compare all this different configurations on the same datasets.

As for the activation, the usage sigmoid resulted in lower overall accuracy (36.95 \% - 41.89 \%). Hyperbolic tangent and ReLU performed better (36.3 \% - 42.29 \% for tanh and 37.26 \% - 42.46 \%). With different optimizers, the differences were negligible. In the end, we chose Adam optimizer. In conclusion, the choice of activation nor the optimizer did not rapidly impact the overall accuracy. 

The choice of topology affected the accuracy more significantly. The sparse models with the lower number of neurons in the hidden layers (less than 32) performed worse (less than or around 40 \%). When adding the layer with at least one dense layer of 256+ neurons, the models usually reached the accuracy of up to 42 \%, no matter the activation.

\subsection*{Training fine-tuning}

We generated our training / valid and test data in the ratio of 60:20:20. All of these sets were disjoint. Training and validation sets were used during the training phase and the independent test set for the final evaluation and overall accuracy. However, even if the overall accuracy reached 40 \%, it happened, that the classifier was poorly trained, especially when the probability distribution of labels over the training set was not uniform. The huge class 13 overshadowed all the other classes. Based on the confusion matrix, we replicated the inferior entries in the training data to obtain uniform distribution. Experimenting with different uniform datasets, we set the optimal number of training epochs to 4. In the later epochs, the models were already overfitting the training data.

Given this adjustments, we were able to train models to perform with an overall accuracy of more than 43 \%. In the figure \ref{comparison-results} we can see the comparison of the CROCS results and our model. Accuracy table consists of rows representing the individual groups, the bigger columns saying whether the group correctly classified within the top 1 (2, 3) results from the classifier and the smaller columns saying how many keys were used by the neural network. We can see that on some groups (like groups 6, 9, 11, 12, 13) it performed better than the previous classifier. On the other hand, it struggled with group 4.

The model had one hidden layer of 256 ReLU neurons, the output layer with softmax activation and used categorical cross-entropy for a loss function. The training dataset has more than 1 million keys, with validation and test data of around 350 thousand keys. The model took around 2 hours to train on Metacentrum.

\begin{sidewaysfigure}[htbp]

\centering
\includegraphics[width=\textwidth]{tex/images/results/comparison_13}
\caption{Comparison of results of CROCS lab and our model.}
\label{comparison-results}

\end{sidewaysfigure}

\noindent
When comparing the Naive Bayes model and the optimized neural network we can see, that:

\begin{itemize}

\item neural network model was able to adapt to the more complex dataset, when compared the scikit models. That is why the best-evaluated scikit model lags behind simple keras classifiers in accuracy.

\item our classifier was able to increase the overall accuracy by almost 3 \% when compared to the methods used by CROCS lab and almost 10 \% when compared to the best results achieved in \cite{thesis_sekan}.

\item a neural network is substantially better in classifying Group 9 (Yubikey 1 \& Infineon JTOP 80K). Given the recent problems with Infineon keys \cite{svenda_2}, discovering their keys with high probability could have been used for potential attack.

\item it performs slightly better on groups 8, 10, 11, 12, 13. 

\item when using a batch of 10+ keys, it can better differentiate groups 11, 12, beating Naive Bayes by 15 - 19 \% (group 11 contains very popular libraries like Crpyto++ and Microsoft crypto libraries).

\item when using a batch of 5+ keys, it can better differentiate group 13 beating Naive Bayes by 17.64 \% with accuracy 73.2 \%. The group 13 contains the highest number of sources, so the bigger success rate applies to a bigger set of sources.

\item the network has really struggled with the group 4. The main reason may have been insufficient data for this group (around 50K for 1024b and 200K for 512b). Therefore it was difficult to generate a balanced dataset containing this group.

\end{itemize}

\subsection*{Binary classifiers}

Based on the previous general model results we can see, that some groups can be detected with high probability even using one single key. We tried to train binary classifiers to differentiate these classes better:

\begin{figure}[H]

%first line
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=\textwidth]{tex/images/results/rese_g1}  
\end{subfigure}%
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=\textwidth]{tex/images/results/rese_g2}
\end{subfigure}
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=\textwidth]{tex/images/results/rese_g3}
\end{subfigure}

\vspace{3mm}
%second line
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=\textwidth]{tex/images/results/rese_g4}  
\end{subfigure}%
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=\textwidth]{tex/images/results/rese_g5}
\end{subfigure}
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=\textwidth]{tex/images/results/rese_g6}
\end{subfigure}

\vspace{3mm}
%third line
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=\textwidth]{tex/images/results/rese_g7}  
\end{subfigure}%
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=\textwidth]{tex/images/results/rese_g8}
\end{subfigure}
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=\textwidth]{tex/images/results/rese_g9}
\end{subfigure}

\vspace{3mm}
%fourth line
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=\textwidth]{tex/images/results/rese_g10}  
\end{subfigure}%
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=\textwidth]{tex/images/results/rese_g11}
\end{subfigure}
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=\textwidth]{tex/images/results/rese_g12}
\end{subfigure}

\vspace{3mm}
%last line
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=\textwidth]{tex/images/results/rese_g13}  
\end{subfigure}%

\caption{Confusion matrices and metrics for 13 classifiers for each group trained individually.}

\end{figure}

\noindent
These results are comparable to the observations from the previous trial:

\begin{itemize}

\item the network completely fails to detect group 4 (even worse than random guessing)
\item it is more or less equal to random guessing with group 11
\item groups 1, 2, 5, 6, 7, 10 and 13 have relatively low precision leading to the higher number of false positives
\item the results for the biggest groups 7, 12 and 13 are more promising. It shows, that we can differentiate them better with accuracy and precision above 65 to 70 \%
\item group 2 can be well distinguished
\item group 8 is detected reliably (the reason is the first MSB being 0)
\item group 9 is detected reliably (the reasons are the moduli 11, 13, 17 and 19). However, the binary classifier overfitted to the small dataset introducing some ratio of false positives.

\end{itemize}

Given the fact that every classifier can be trained individually, we obtained the best results when putting them all together in the voting process. The same input was fed to 13 different binary classifiers, that outputted the probability, that the key belongs to the respective group. The classifier with the highest probability was voted as a winner.

We ran the voting classifier on the uniformly distributed dataset of 13 groups of bit length 1024. Table \ref{table-binary-voter} shows the obtained results. The notable observations of this run are:

\begin{itemize}

\item the overall accuracy is 4.94 \% higher than naive Bayes
\item the classifier gets more successful in identifying the large group 13, with the accuracy of 22.95 \% on the first guess and 73.21 \% when considering first top 3 results. This is almost 18 \% better than the result of naive Bayes and 12 \% better than our previous model.
\item it can detect group 8 with more accuracy (3.5 \% higher)
\item it overfitted group 9 (because of its size). Nevertheless, we identified, that this is the only group, that has bias modulo 11, 13, 17 and 19. We suppose that this classifier would perform well even on a bigger sample of this group\footnote{Group 9 contains Infineon chips, which were liable to attack as reported in \cite{svenda_2}. The correct classification of such group could be useful for the potential attacker.}. 

\end{itemize}

\begin{table}[h]
\centering
\begin{tabular}{| r | c | c | c | r | }
\hline
  Group & Top 1 match & Top 2 match & Top 3 match & \# of keys \\
  \hline
 1 & 58.30 \% & 89.20 \% & 97.06 \% & 93482 \\
 2 & 85.57 \% & 98.02 \% & 99.62 \% & 136727 \\
 3 & 15.28 \% & 32.07 \% & 52.50 \% & 200000 \\
 4 & 47.96 \% & 59.15 \% & 64.03 \% & 50000 \\
 5 & 39.82 \% & 69.73 \% & 85.47 \% & 200000 \\
 6 & 49.81 \% & 74.32 \% & 86.73 \% & 150000 \\
 7 & 55.21 \% & 76.15 \% & 90.36 \% & 200000 \\
 8 & 93.54 \% & 94.81 \% & 95.74 \% & 200000 \\
 9 & 100 \% & 100 \% & 100 \% & 114955 \\
 10 & 33.22 \% & 57.60 \% & 79.44 \% & 129347 \\
 11 & 0.34 \% & 23.59 \% & 47.16 \% & 189110 \\
 12 & 25.04 \% & 28.95 \% & 42.07 \% & 199990 \\
 13 & 22.95 \% & 53.84 \% & 73.21 \% & 191658 \\
 \hline
TOTAL & 45.28 \% & 63.21 \% & 76.32 \% & 2055269 \\
\hline

\end{tabular}
\caption{Accuracy table for a voting classifier of binary classifiers.}
\label{table-binary-voter}
\end{table}

The voting classifier performs better in almost every aspect than the previous one apart from groups 6, 10 and 11, for which the user could use rather the first one. If we also included the CROCS's naive bayes classifier for classifying group 2 (in which it is superior to both of our classifiers), we would get even more accurate predictions.

\subsection*{Group spliting}

As the last step, we tried even more precise classification between sources. We aimed to split at least one of the 13 groups. In the first approach, we ran the models on the huge dataset with 64 individual sources. In the second one, we took sources only from a single group and tried to classify them within it. We repeated it for each group.

However, when running several proven models on each group, the results showed as expected the uniform distribution in classification within the group, as the sources in one group shared the same distribution on the features. The network is not able to differentiate between them and is performing with the same results as random guessing. We suppose that the sources within each group share the same implementation steps that affect the distribution on bits in the public key and the distribution on the remainders modulo up to 30.

