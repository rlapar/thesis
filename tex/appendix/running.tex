\chapter{Examples of application commands}

\label{appendix-running}

In this appendix you can found examples of how to run various commands on the application. To run general help:

\begin{minted}[ framesep=2mm,
                autogobble,
                frame=lines]{shell}

python3 cli.py --help

\end{minted}

\section*{ANALYZE}

To run help about task:

\begin{minted}[ framesep=2mm,
                autogobble,
                frame=lines]{shell}

python3 cli.py analyze --help-task

\end{minted}

\noindent
The example below show the usage of the analyze task. First of all, setup YAML:

\begin{minted}[ framesep=2mm,
                autogobble,
                frame=lines]{yaml}

common:
  batch_size: 500000
  keys_per_feature: 1
...  
fm:
  key_length: 1024
  binary_only: 0
  features:
  - ['mod', {'n': 3}]
  - ['mod', {'n': 4}]
  - ['mod', {'n': 5}]

\end{minted}

\noindent
Then run command:

\begin{minted}[ framesep=2mm,
                autogobble,
                frame=lines]{shell}

cli.py analyze --analyzed-csv <PATH/TO/CSV> \
--analysis-output <PATH/TO/OUTPUT> \
--json-format

\end{minted}

\noindent
This will run the analyze task and on the \texttt{<PATH/TO/CSV>} file, that have 1 key per line of length 1024 bits in batches of 500000 lines. The analyzed features are moduli of 3, 4 and 5. The output is saved in \texttt{<PATH/TO/OUTPUT>} json file. The output can look like:

\begin{minted}[ framesep=2mm,
                autogobble,
                frame=lines]{json}

"feature_counter": {
  ...
  "2": {
    "('mod', 5)": {
      "[1]": 16821,
      "[2]": 11206,
      "[3]": 11006,
      "[4]": 10967      
    },
    "('mod', 4)": {
    ...  
  },
  ...
},
"group_counter": {
  ...
  "('mod', 5)": {
    "[1]": {
      "1": 23371,
      "2": 16821,
      ...
    },
    ...
  },
  ...
}  

\end{minted}

\noindent
Feature counter shows the counts for features per each group (\texttt{group.feature.value}). Here we can see, that group 2 has bias towards the remainder 1 when divided by 5. The group counter shows the counts for groups per each feature (\texttt{feature.value.group}). Here we can see, that even though the group 2 has bias towards 1 and group 1 is uniformly distributed modulo 5, there is higher chance of getting group 1 than 2. The result could mean, that the neural network would be biased towards the group 1, if this would be the training data.


\section*{GENERATE}

To run help about task:

\begin{minted}[ framesep=2mm,
                autogobble,
                frame=lines]{shell}

python3 cli.py generate --help-task

\end{minted}

\noindent
The example below was used to generate a uniform dataset for 13 groups from our  modified dataset. YAML configuration:

\begin{minted}[ framesep=2mm,
                autogobble,
                frame=lines]{yaml}

common:
  batch_size: 1000000
  keys_per_feature: 1
  valid_ratio: 0.2
  test_ratio: 0.2
  number_of_groups: 64 # the original dataset has 64 sources
...  
generate:
  keys_per_group: 200000
  skipped_groups: []
  skipped_files: []
  train_keys_multiplicity: # replicate training data in small sources
    1: 2
    2: 2
    5: 4
    17: 2
    18: 3
    21: 5
  merge_groups: # merge sources in the same group
  - [2,3]
  - [6,7,8,9]
  - [11,12,13,14]
  - [19,20,21]
  - [22,23,24,25,26,27,28,29]
  - [30,31,32,33,34,35,36,37,38,39,40]
  - [41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64]
\end{minted}

\noindent
To generate the new dataset we used the JSON file for 1024 bit sources with command:

\begin{minted}[ framesep=2mm,
                autogobble,
                frame=lines]{shell}

cli.py generate --sources-path sources/grouped_1024.json --dataset \
--datapath <PATH_TO_NEW_DATASET> --shuffle --relabel 

\end{minted}

\section*{TRAIN}

To run help about task:

\begin{minted}[ framesep=2mm,
                autogobble,
                frame=lines]{shell}

python3 cli.py train --help-task

\end{minted}

\noindent
The following example shows configuration from our train runs. YAML:

\begin{minted}[ framesep=2mm,
                autogobble,
                frame=lines]{yaml}

common:
  batch_size: 2048
  keys_per_feature: 1
  valid_ratio: 0.2
  test_ratio: 0.2
  number_of_groups: 13 # newly generated 13 sources
...  
train:
  epochs: 4
fm:
  key_length: 1024
  binary_only: 1
  features:
  - 'line'
  - 'mod3'
  - 'mod4'
  - ['mod', {'n': 5}]
  - ...
\end{minted}

\noindent
Command:

\begin{minted}[ framesep=2mm,
                autogobble,
                frame=lines]{shell}

python3 cli.py train --dataset --incremental \
--datapath <PATH_TO_NEW_DATASET> \
--model Keras28

\end{minted}

\noindent
Model topology configuration:

\begin{minted}[ framesep=2mm,
                autogobble,
                frame=lines]{json}

"Keras28": {"base": "KerasDeepClassifier",
             "name": "Keras ReLU Deep NN",
             "topology": {"hidden_layers": [{"activation": "relu",
                                             "number_of_neurons": 128},
                                            {"activation": "relu",
                                             "number_of_neurons": 1024}],
                          "loss": "categorical_crossentropy",
                          "metrics": ["acc"],
                          "optimizer": "Adadelta",
                          "output_layer": {"activation": "softmax"}}},

\end{minted}

The results are saved in \texttt{results} directory with incrementing \texttt{run\_id}. Each run stores \texttt{classifier}, \texttt{config.txt} (configuration from YAML), \texttt{run\_info.txt} (information about dataset, features, model and time of run) and \texttt{results.txt} (evaluation - confusion matrix and accuracy table).

\section*{CLASSIFY}

To run help about task:

\begin{minted}[ framesep=2mm,
                autogobble,
                frame=lines]{shell}

python3 cli.py classify --help-task

\end{minted}

\noindent
The YAML configuration is similar to train task (fileds \textit{common} and \texttt{fm}). To run the classifier on custom dataset and evaluate its performance:

\begin{minted}[ framesep=2mm,
                autogobble,
                frame=lines]{shell}

python3 cli.py classify --classifier <PATH_TO_CLASSIFIER> \
--source-csv <PATH_TO_CSV> --compare-labels

\end{minted}

\noindent
To run the classifier on custom dataset and assign new labels, one does need to provide the path to the newly labeled dataset:

\begin{minted}[ framesep=2mm,
                autogobble,
                frame=lines]{shell}

python3 cli.py classify --classifier <PATH_TO_CLASSIFIER> \
--source-csv <PATH_TO_CSV> --assign-labels \
--target-csv <PATH_TO_LABELED_CSV>

\end{minted}

\section*{Running in docker environment}

When not on a Debian machine, one can run the application in the docker environment. The user has to bind the used volumes for dataset, results and generated data. User then build the container and runs it interactivelly by the following commands:

\begin{minted}[ framesep=2mm,
                autogobble,
                frame=lines]{shell}

docker build . -t rsaml --file Dockerfile

docker run -v <path_to_dataset>:/opt/dataset \
-v <path_to_results>:/opt/rsa-ml/results \
-v <path_to_generated_datasets>:/opt/rsa-ml/datasets \
-i -t rsaml:latest /bin/bash

\end{minted}

\section*{Running on metacentrum}
\label{appendix-metacentrum}

To run the application on Metacenturm we run the shell script, that:

\begin{itemize}

\item specified desired resources (number of processors, memory, scratch memory and maximum runtime)

\item loaded the necessary dependencies

\item installed third party modules locally (in our case \texttt{click})

\item ran the application

\end{itemize}

\noindent
The example of used script is showed below:

\begin{minted}[ framesep=2mm,
                autogobble,
                frame=lines]{shell}

#PBS -l select=1:ncpus=1:mem=8gb:scratch_local=4gb
#PBS -l walltime=3:00:00

# sets home directory
DATADIR="/storage/brno6/home/xlapar"

cd $DATADIR/git/rsa-ml/

module add tensorflow-1.5.0-cpu-python3
module add python34-modules-gcc

# install click
export PYTHONUSERBASE=$DATADIR/.local
export PATH=$PYTHONUSERBASE/bin:$PATH
export PYTHONPATH=$PYTHONUSERBASE/lib/python3.4/site-packages:$PYTHONPATH
pip install click --user --process-dependency-links

# export for click
export LC_ALL=C.UTF-8
export LANG=C.UTF-8

python3 $DATADIR/git/rsa-ml/cli.py train \
--dataset --incremental \
--datapath $DATADIR/data/datasets/dataset_1 \
--settings $DATADIR/settings/generate/groups_13/1024.yaml \
--model $MODEL

\end{minted}









